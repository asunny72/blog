{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch\n",
    "\n",
    "<img style=\"float: left; padding-right:0.5em;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/320px-MnistExamples.png\">\n",
    "\n",
    "This notebook trains a simple artificial neural network from end-to-end with numpy.\n",
    "\n",
    "We will train a model to classify handwritten digits [0-9] using the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database). This is the \"Hello World!\" example for neural network based Computer Vision (CV) models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Download `train.csv` from [this kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/data), and set the `TRAIN_DATA_PATH` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"/your-path/train.csv\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH)\n",
    "data = np.array(train_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image captures a handwritten digit and has a resolution of `28 X 28` pixels, for a total of 784 pixels per image. Each pixel has a single value associated with it from [0, 255], which indicates its darkness.\n",
    "\n",
    "`train.csv` represents each image as a single row in an `M x 785` matrix, where M is the number of images, and the number of columns consists of a) the number of pixels (784) per image and b) the label that denotes what the digit actually is.\n",
    "\n",
    "- matrix[0] = [ label, pixel0, pixel1, pixel2, pixel3 ]\n",
    "- image = matrix[0][1:] = [ pixel0, pixel1, pixel2, pixel3]\n",
    "- label = matrix[0][0]\n",
    "\n",
    "For example, let's look at a 2x2 image to understand how to \"Unflatten\" the 1D vector to the 2D matrix:\n",
    "\n",
    "<img src=\"https://assets.leetcode.com/uploads/2021/08/26/image-20210826114243-1.png\" alt=\"1D to 2D\" width=\"436\" height=\"151\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_image(flattened: np.array, m=28, n=28):\n",
    "    assert flattened.ndim == 1, \"input must be one dimensional\"\n",
    "    matrix = flattened.reshape(m, n)\n",
    "    plt.figure(figsize=(2.5, 2.5))\n",
    "    plt.imshow(matrix, interpolation=\"nearest\")\n",
    "    plt.gray()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show_image(data[random.randint(0, len(data))][1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learned Features vs Handcrafted Features\n",
    "\n",
    "Let's brainstorm approaches to build a digit classifier w/out fancy ML. \n",
    "\n",
    "For example, we can break an input into distincitve `features` such as number of lines and loops([circle hough transform](https://en.wikipedia.org/wiki/Circle_Hough_Transform)). The digit '0' yields one loop loop, while the digit '8' yields two loops. The digits '9' and '6' both yield one loop and one line. Next, we can consider the spatial relation between these lines and loops. If the loop is on the bottom, it's a '6'.\n",
    "\n",
    "Essentially, we just hand crafted \"features\", and composed rules to judge the final result based on the feature values . The quality of our algorithm is directely porportional to how well crafted each feature is.\n",
    "\n",
    "In the above example, we hand crafted features, and as a result, could easily understand them. Alternatively, we can employ an artificial neural network to self learn all of the important features. While these features may not be interperable by humans, they certainly are distictive to the neural network. Let's further explore a neural network below.\n",
    "\n",
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "<img style=\"padding-right:0.5em;\" src=\"https://res.cloudinary.com/practicaldev/image/fetch/s--2UPg0Z-6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://imgur.com/KeIJEYy.jpeg\">\n",
    "\n",
    "The neural network architecture we will explore is the multi-layer perceptron (MLP), depicted above. Each node represents a neuron. Each column is an arrangement of neurons represent a layer.\n",
    "\n",
    "The layer on the far left is dubbed the \"input layer\". The layer on the far right is the \"output layer\". The layers in between are the \"hidden layers\".\n",
    "\n",
    "Each neuron in the input layer represents a pixel of the image, hence, there should by 784 inputs. The output layer should have 10 neurons, that correspond to a digit between [0, 9]. The number of hidden layers can be from [1, inf). We tune the number of hidden layers, and number of neurons per hidden layer empirically through experimentation.\n",
    "\n",
    "### A fictional thought excercise on neurons \"firing\"\n",
    "\n",
    "Imagine a human brain where certain neurons \"light up\" or \"activate\" based on observed inputs. E.g., some grouping of neurons light up when you smell pizza. In the above image, seeing a '9' lights up a certain pattern of neurons in the first hidden layer. The 2nd hidden layer lights up in response to the input pattern it observes in the first hidden layer. The output layer responds to how the neurons in the 2nd hidden layer are \"activated\".\n",
    "\n",
    "The next few sentences are FICTIONAL, but hope to help provide an analagy to better understand what the neural network is doing. Rembemer the analagy above, where to detect a '9' we can see if there is a loop co-joined with a line? We can imagine (again, FICTIONAL) that the first layer of this network is finds the loops and lines. If the neurons in that layer light up in a certain way, that means we detected a some number of loops and or lines. The next layer interprets this, and is responsible for seeing how close the components are together. If the loop is near the top, and there is a vertical line immediately to the right of the loop, the output neurons light up a certain way, and in this case, ONLY the output node that corresponds to the '9' lights up.\n",
    "\n",
    "Again, this isn't what is actually happening. The machine learns its own features that may not be human understandable. What's important to note is that the neurons in the preceding layer influence the neurons in the next layer. The first hidden layer recognizes patterns in the input. The 2nd hidden layer recognizes patterns in the 1st hidden layer. The output recognizes patterns in the 2nd hidden layer.\n",
    "\n",
    "We will implement this structure to see how well it performs. But first, let's understand the basic constituent units.\n",
    "\n",
    "### Artificial Neurons\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/338190342/figure/fig2/AS:840701659074561@1577450303999/Structures-of-artificial-neural-network-ANN-model-that-show-a-data-flows-in-the-ANN.png\">\n",
    "\n",
    "An artificial neuron is the base unit that makes up most (if not all) neural network based algorithms.\n",
    "\n",
    "In the MLP, Each neuron consists of two components a) a linear combination of weighted inputs that feed into b) an activation function that outputs a value between 0 and 1.\n",
    "\n",
    "Let's take a look below at the \"perceptron\", invented by Frank Rosenblatt in 1957. The perceptron uses the \"threshold\" activation function. If the weighted sum of inputs exceeds a given threshold, the neuron \"activates\".\n",
    "\n",
    "<img src =\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*FBJqWE6AHt8HMZ7XHW9uWg.png\">\n",
    "<!-- <img src=\"https://www.researchgate.net/publication/281953313/figure/fig1/AS:668914816139278@1536493128709/Architecture-of-the-three-layer-multilayer-perceptron-MLP-neural-network_Q640.jpg\"> -->\n",
    "\n",
    "### Sigmoid Neuron - a smoothed perceptron\n",
    "\n",
    "The threshold function does not provide granular feedback. Small changes in inputs either result in no changes, or a binary 0->1 change. This property complicates functions that find optimal values of the weights (more on this later). We can solve this by using the sigmoid activation function instead. We can conceptualize it as a smoothed threshold function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sigmoid(X)')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 1000)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "plt.plot(x, sigmoid, linestyle=\"solid\")\n",
    "plt.ylabel(\"Sigmoid(X)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the similarity between the sigmoiod and threshold functions. At extreme inputs, they behave similiarly. In the above graph, values around (-2.5, 2.5) behave differenly from the threshold function. Now the output can be any value between 0 and 1, and small changes in the weights and biases can lead to small changes in the output.\n",
    "\n",
    "The MLP actually uses sigmoid neurons, as opposed to perceptrons like its namesake. We will use the sigmoid in our implementation. \n",
    "\n",
    "### Softmax\n",
    "We will use the sigmoid activation function on all hidden layers. However, on the last layer, we will use a **softmax** activation function.\n",
    "\n",
    "![softmax equation](https://www.gstatic.com/education/formulas2/472522532/en/softmax_function.svg)\n",
    "\n",
    "A softmax function normalizes the input into a probability distribution of K classes. In otherwords, in our last layer, we don't want postive or negative values.\n",
    "\n",
    "We want to know the probability the input is an '0', '1', '2', '3'... or a '9'. These probabilities should sum to one."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Implement!\n",
    "\n",
    "We are going to create the simplest MLP possible => input layer to one hidden layer => output layer.\n",
    "\n",
    "We will visualize each neuron as having two components: a) the weighted sum, and b) the activation function. However, in the implementaion, we will separate these steps.\n",
    "\n",
    "So visually, the neural network will look something like below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_logs\n",
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.48.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"443pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 443.37 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 439.37,-40 439.37,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"167.34\" cy=\"-18\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"167.34\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z1 | A1 (Sigmoid)</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.02,-18C61.67,-18 70.48,-18 79.74,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.93,-21.5 89.93,-18 79.93,-14.5 79.93,-21.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"358.03\" cy=\"-18\" rx=\"77.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"358.03\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z2 | A2 (Softmax)</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.82,-18C253.17,-18 261.71,-18 270.18,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"270.45,-21.5 280.45,-18 270.45,-14.5 270.45,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x154f77d60>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "nn_vis = Digraph()\n",
    "nn_vis.graph_attr[\"rankdir\"] = \"LR\"\n",
    "nn_vis.node(\"X\", \"X\")\n",
    "nn_vis.node(\"1\", \"Z1 | A1 (Sigmoid)\")\n",
    "nn_vis.node(\"2\", \"Z2 | A2 (Softmax)\")\n",
    "\n",
    "nn_vis.edges([\"X1\", \"12\"])\n",
    "nn_vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But programatically, the pipeline looks more like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.48.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"860pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 860.44 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 856.44,-40 856.44,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"46.15\" cy=\"-18\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"46.15\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X = Input</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"219.93\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.93\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z1 = Weighted Sum 1</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.73,-18C100.75,-18 109.37,-18 118.21,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.26,-21.5 128.26,-18 118.26,-14.5 118.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"409.97\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.97\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A1 = Sigmoid</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M311.65,-18C320.26,-18 328.91,-18 337.31,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337.41,-21.5 347.41,-18 337.41,-14.5 337.41,-21.5\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"600.01\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.01\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z2 = Weighted Sum 2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.54,-18C480.78,-18 489.39,-18 498.09,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"498.31,-21.5 508.31,-18 498.31,-14.5 498.31,-21.5\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"790.04\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"790.04\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A2 = Softmax</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M691.72,-18C700.33,-18 708.98,-18 717.38,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"717.48,-21.5 727.48,-18 717.48,-14.5 717.48,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x154f778e0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_impl_vis = Digraph()\n",
    "nn_impl_vis.graph_attr[\"rankdir\"] = \"LR\"\n",
    "nn_impl_vis.node(\"X\", \"X = Input\")\n",
    "nn_impl_vis.node(\"1\", \"Z1 = Weighted Sum 1\")\n",
    "nn_impl_vis.node(\"2\", \"A1 = Sigmoid\")\n",
    "nn_impl_vis.node(\"3\", \"Z2 = Weighted Sum 2\")\n",
    "nn_impl_vis.node(\"4\", \"A2 = Softmax\")\n",
    "\n",
    "nn_impl_vis.edges([\"X1\", \"12\", \"23\", \"34\"])\n",
    "nn_impl_vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X = Input Layer\n",
    "\n",
    "784 neurons, one for each pixel in any given input image\n",
    "\n",
    "Shape: (784, n)\n",
    "\n",
    "### Z1 = Hidden Layer 1 -> Weighted Sum\n",
    "\n",
    "$ Z1 = W1 * X + b1 $\n",
    "\n",
    "For now, we will randomly set values for W1 and B1. The size of this layer is 10.\n",
    "\n",
    "Shape:\n",
    "\n",
    "- W1: (10, 784)\n",
    "- b1: (10, 1)\n",
    "\n",
    "### A1 = Hiddel Layer 1 -> Activation Function\n",
    "\n",
    "$ A1 = Sigmoid(Z1) $\n",
    "\n",
    "Remember each neuron consists of both weighted sum and activation function. So we should think of A1 and Z1 as one group.\n",
    "\n",
    "Shape: (10, n)\n",
    "\n",
    "### Z2 = Output Layer -> Weighted Sum\n",
    "\n",
    "$ Z2 = W2 * A1 + b2 $\n",
    "\n",
    "10 layers, one for each digit [0,9]\n",
    "\n",
    "Shape: (10, n)\n",
    "\n",
    "### A2 = Output Layer -> Activation Function\n",
    "\n",
    "We use the softmax function (instead of the same activation function as A1) in order to ensure the final output is a probability that the input is one of the given options.\n",
    "\n",
    "$ A2 = softmax(Z2) $\n",
    "\n",
    "The output should look something like this:\n",
    "\n",
    "```\n",
    "predicted:  {\n",
    "    \"0\": 0.002285052335889073,\n",
    "    \"1\": 0.0021112562178389332,\n",
    "    ...\n",
    "    \"8\": 0.12941646239061635,\n",
    "    \"9\": 0.5603362502515766\n",
    "}\n",
    "```\n",
    "\n",
    "Shape: (10, n)\n",
    "\n",
    "### Commonly used conventions -> Y_hat\n",
    "\n",
    "The '' is often used in the literature to denote the prediction.\n",
    "\n",
    "$ A2 = Y_{hat} $\n",
    "\n",
    "### Human readable output\n",
    "\n",
    "We will use the symbol $ Y\\_{hum} $. This is not a common convention.\n",
    "\n",
    "$ Y_{hum} = argmax(Y_{hat}) $\n",
    "\n",
    "### End to End\n",
    "\n",
    "$\n",
    "Y_{hat} = A2 \\\\\n",
    "Y_{hat} = softmax(Z2) \\\\\n",
    "Y_{hat} = softmax(W2 * A1 + b2) \\\\\n",
    "Y_{hat} = softmax(W2 * Sigmoid(Z1) + b2) \\\\\n",
    "Y_{hat} = softmax(W2 * Sigmoid(W1 * X + b1) + b2)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.48.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"860pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 860.44 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 856.44,-40 856.44,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"46.15\" cy=\"-18\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"46.15\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X = Input</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"219.93\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.93\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z1 = Weighted Sum 1</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.73,-18C100.75,-18 109.37,-18 118.21,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.26,-21.5 128.26,-18 118.26,-14.5 118.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"409.97\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.97\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A1 = Sigmoid</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M311.65,-18C320.26,-18 328.91,-18 337.31,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337.41,-21.5 347.41,-18 337.41,-14.5 337.41,-21.5\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"600.01\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.01\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z2 = Weighted Sum 2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.54,-18C480.78,-18 489.39,-18 498.09,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"498.31,-21.5 508.31,-18 498.31,-14.5 498.31,-21.5\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"790.04\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"790.04\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A2 = Softmax</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M691.72,-18C700.33,-18 708.98,-18 717.38,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"717.48,-21.5 727.48,-18 717.48,-14.5 717.48,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x154f778e0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_impl_vis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Preprocess Data\n",
    "1. Split `train.csv` into the training dataset, and the validation dataset. We will fit a model to the training data and measure \n",
    "performance on the validation data.\n",
    "\n",
    "2. Normailze values to be between [0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.shuffle(data)\n",
    "m, n = data.shape\n",
    "num_examples, num_pixels = m, n - 1\n",
    "\n",
    "validation_data_size = 10000\n",
    "validation_data = data[0:validation_data_size].T\n",
    "train_data = data[validation_data_size:].T\n",
    "\n",
    "\n",
    "def get_x_y(matrix):\n",
    "    def normalize(input):\n",
    "        return input / 255.0\n",
    "\n",
    "    X = normalize(matrix[1:])\n",
    "    Y = matrix[0]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X_val, Y_val = get_x_y(validation_data)\n",
    "X_train, Y_train = get_x_y(train_data)\n",
    "\n",
    "show_image(X_train.T[random.randint(0, len(X_train))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Initialize Weights and Biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    \"\"\"\n",
    "    rand returns values between [0, 1). Subtract 0.5 to ensure symmetrical range from [-0.5, 0.5)\n",
    "    \"\"\"\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "W1, b1, W2, b2 = init_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.48.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"860pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 860.44 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 856.44,-40 856.44,4 -4,4\"/>\n",
       "<!-- X -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"46.15\" cy=\"-18\" rx=\"46.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"46.15\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X = Input</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"219.93\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.93\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z1 = Weighted Sum 1</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>X&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.73,-18C100.75,-18 109.37,-18 118.21,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"118.26,-21.5 128.26,-18 118.26,-14.5 118.26,-21.5\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"409.97\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"409.97\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A1 = Sigmoid</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M311.65,-18C320.26,-18 328.91,-18 337.31,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"337.41,-21.5 347.41,-18 337.41,-14.5 337.41,-21.5\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"600.01\" cy=\"-18\" rx=\"91.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"600.01\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Z2 = Weighted Sum 2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M472.54,-18C480.78,-18 489.39,-18 498.09,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"498.31,-21.5 508.31,-18 498.31,-14.5 498.31,-21.5\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"790.04\" cy=\"-18\" rx=\"62.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"790.04\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">A2 = Softmax</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M691.72,-18C700.33,-18 708.98,-18 717.38,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"717.48,-21.5 727.48,-18 717.48,-14.5 717.48,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x154f778e0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_impl_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input):\n",
    "    return 1 / (1 + np.exp(-input))\n",
    "\n",
    "\n",
    "def softmax(input):\n",
    "    return np.exp(input) / np.exp(input).sum(axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "def get_human_readable_prediction(Y_hat):\n",
    "    return np.argmax(Y_hat, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    Y_hat = softmax(Z2)\n",
    "    return Z1, A1, Z2, Y_hat\n",
    "\n",
    "\n",
    "Z1, A1, Z2, Y_hat = forward(X_train, W1, b1, W2, b2)\n",
    "Y_hum = get_human_readable_prediction(Y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Audit Results\n",
    "\n",
    "What do you think our accuracy is given we randomally initialized the weights and biases?\n",
    "\n",
    "Let's first look at the predicted result, and the actual result. Note that we will express the actual result in a \"one-hot\" encoded format.\n",
    "\n",
    "In one-hot encodings, only one position is \"hot\", while all others are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  {\n",
      "    \"0\": 0.05905948783152979,\n",
      "    \"1\": 0.06274394722871603,\n",
      "    \"2\": 0.1430820575032908,\n",
      "    \"3\": 0.16667098372824396,\n",
      "    \"4\": 0.0785915999896603,\n",
      "    \"5\": 0.0565904123259883,\n",
      "    \"6\": 0.3071298263639981,\n",
      "    \"7\": 0.06920830156606073,\n",
      "    \"8\": 0.015179435852722084,\n",
      "    \"9\": 0.04174394760978998\n",
      "}\n",
      "actual:  {\n",
      "    \"0\": 0.0,\n",
      "    \"1\": 0.0,\n",
      "    \"2\": 0.0,\n",
      "    \"3\": 0.0,\n",
      "    \"4\": 1.0,\n",
      "    \"5\": 0.0,\n",
      "    \"6\": 0.0,\n",
      "    \"7\": 0.0,\n",
      "    \"8\": 0.0,\n",
      "    \"9\": 0.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "digit_to_prob = dict([(i, p) for i, p in enumerate(list(Y_hat.T[0]))])\n",
    "print(\"predicted: \", json.dumps(digit_to_prob, indent=4))\n",
    "\n",
    "answer = dict([(i, 0.0) for i in range(0, 10)])\n",
    "answer[Y_train[0]] = 1.0\n",
    "print(\"actual: \", json.dumps(answer, indent=4))\n",
    "show_image(X_train.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.09434375\n",
      "Predicted:  [6 6 6 ... 6 6 6]\n",
      "Actual:  [4 0 5 ... 8 5 9]\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(Y_hat_hum, Y_truth_hum):\n",
    "    assert len(Y_hat_hum) == len(Y_truth_hum)\n",
    "    return np.sum(Y_hat_hum == Y_truth_hum) / Y_truth_hum.size\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", get_accuracy(Y_hum, Y_train))\n",
    "print(\"Predicted: \", Y_hum)\n",
    "print(\"Actual: \", Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Success!\n",
    "\n",
    "We just successfully implemented an MLP! Its performance is literally random, but that's okay! The structure is 100% reminiscent of a model you would deploy in prodcution for inference!\n",
    "\n",
    "### Conceptual Checkpoint\n",
    "\n",
    "1. If our co-worker from a different team wants us to share our model so that they can use it on their own dataset, at the bare minimum, what do we need to share?\n",
    "\n",
    "2. What parts of the MLP are highly reusable? And what parts are very application specific?\n",
    "\n",
    "3. What are alternative ways to set the weights to do better than random?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going back to tune the weights and biases\n",
    "\n",
    "Brief aside to an analagy. Imagine you are trying to find the perfect water temperature for a shower.\n",
    "\n",
    "![shower lever](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT7fVauofLAnNZ2cPFCGkhQkoXufZJm7oQ_MIcqwXOKhLQqEo46V3LKpJH7h-VPHVIweZI&usqp=CAU)\n",
    "\n",
    "1. You start by moving the knob to some position you think will work\n",
    "2. You turn on the water\n",
    "3. You test the result by feeling the water with your hand\n",
    "4. You update the lever based on what you felt.\n",
    "\n",
    "   a. If the water was WAY too cold, you know to make a large adjustment\n",
    "\n",
    "   b. If the water was a little too cold, you make a small adjustment\n",
    "\n",
    "   c. vice versa for when the water is too hot\n",
    "\n",
    "5. Repeat steps 2-4 until you converge to the perfect water temperature\n",
    "\n",
    "We can think of weights and biases as the shower lever. Let's pretend we have a mechanism that informs us on how much we need to adjust the weights and biases by based on how far off the predictions are from the truth. In this case, our process would be similar to the above shower example. We start with randomly initialized weights. Predict. Measure how bad the accuracy is. Go back and change the weights and biases. Predict. Measure again.\n",
    "\n",
    "This process actually exists! We can use Gradient Descent to find the optimal values for the weights and biases.\n",
    "\n",
    "### Gradient Descent; An Overview\n",
    "Gradient descent, in the context of an artificial network, conists of a few key components:\n",
    "\n",
    "1. Cost function\n",
    "2. Backpropagation to calculate the optimal \"direction\" to step in to minimize the cost.\n",
    "3. Take a small \"step\" in the aforementioned direction\n",
    "4. Repeat until convergence\n",
    "\n",
    "We define a cost function that we want to minimize. We then calculate the change in cost with respect to the weights and biases, aka the gradient. The gradient tells us the direction of the largest ascent. We then take a small step in the opposite direction and update our weights and biases accordingly.\n",
    "\n",
    "![gd flow (should only be rendered if viewing notebook via jupyter server)](./img/gd_flow.png)\n",
    "![gd flow (should only be rendered if viewing on static site)](../img/gd_flow.png)\n",
    "\n",
    "### 1. Cost (aka Loss) Function\n",
    "\n",
    "To update the weights, we first need to understand how much we need to update by (similar to the shower example, this is an informed \"guess\"), and in what direction.\n",
    "\n",
    "Enter the cost (aka loss) function.\n",
    "\n",
    "Let's first look at the Mean Squared Error function. This is NOT the cosst function we will implement, but serves to demonstrate the purpose of a cost function well:\n",
    "\n",
    "![MSE](https://miro.medium.com/v2/resize:fit:640/format:webp/1*SGhoeJ_BgcfqU06CmX41rw.png)\n",
    "\n",
    "Reminder: here is the output of our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted:  {\n",
      "    \"0\": 0.05905948783152979,\n",
      "    \"1\": 0.06274394722871603,\n",
      "    \"2\": 0.1430820575032908,\n",
      "    \"3\": 0.16667098372824396,\n",
      "    \"4\": 0.0785915999896603,\n",
      "    \"5\": 0.0565904123259883,\n",
      "    \"6\": 0.3071298263639981,\n",
      "    \"7\": 0.06920830156606073,\n",
      "    \"8\": 0.015179435852722084,\n",
      "    \"9\": 0.04174394760978998\n",
      "}\n",
      "actual:  {\n",
      "    \"0\": 0.0,\n",
      "    \"1\": 0.0,\n",
      "    \"2\": 0.0,\n",
      "    \"3\": 0.0,\n",
      "    \"4\": 1.0,\n",
      "    \"5\": 0.0,\n",
      "    \"6\": 0.0,\n",
      "    \"7\": 0.0,\n",
      "    \"8\": 0.0,\n",
      "    \"9\": 0.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted: \", json.dumps(digit_to_prob, indent=4))\n",
    "print(\"actual: \", json.dumps(answer, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of what inputs create large MSE values.\n",
    "\n",
    "```\n",
    "predicted:  {\n",
    "    \"0\": 1.0\n",
    "    \"1\": 0.0\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "actual:  {\n",
    "    \"0\": 0.0,\n",
    "    \"1\": 1.0,\n",
    "    \"2\": 0.0,\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "\n",
    "(actual - predicted)^2:\n",
    "{\n",
    "    \"0\": 1.0,\n",
    "    \"1\": 1.0\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "```\n",
    "\n",
    "`sum(actual - predicted)^2 = 2`\n",
    "\n",
    "Observe how vastly wrong predictions result in large values in (actual - predicted). Another nice property of MSE is that we penalize larger differences more severely than small ones since we square the difference, and larger differences degrade exponentially as quickly as smaller differences. Now let's look at an input that creates small MSE values.\n",
    "\n",
    "```\n",
    "predicted:  {\n",
    "    \"0\": 1.0\n",
    "    \"1\": 0.0\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "actual:  {\n",
    "    \"0\": 1.0\n",
    "    \"1\": 0.0\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "\n",
    "(actual - predicted)^2:\n",
    "{\n",
    "    \"0\": 0.0,\n",
    "    \"1\": 0.0\n",
    "    ...\n",
    "    \"9\": 0.0\n",
    "}\n",
    "```\n",
    "\n",
    "`sum(actual - predicted)^2 = 0`\n",
    "\n",
    "Observe how perfect predictions result in 0 when we take (actual - predicted).\n",
    "\n",
    "**Conclusion: we want a small value for the cost function.**\n",
    "\n",
    "In our implementation, we will not be using MSE. Instead, we will use **cross entropy** loss (aka softmax loss aka negative log likelihood) because its derivative combines super nicely with the derivative of the softmax.\n",
    "\n",
    "Cross Entropy Loss:\n",
    "![cross entropy loss](https://www.oreilly.com/api/v2/epubs/9781789130331/files/assets/b53e5bab-5f7a-4cf0-b749-92b36255f365.png)\n",
    "\n",
    "Though the properties of this function are different from MSE, the trends and objectives remains the same. Bad predictions result in large values, and we want to optimize for small values.\n",
    "\n",
    "Note that in our implementation, we will take the mean instead of the sum. Taking the mean is advantagous for adjusting the rest of the system (we won't cover why, but reasons are expanded upon [here](https://stats.stackexchange.com/questions/306862/cross-entropy-versus-mean-of-cross-entropy)). Mathmatically, we can think of the mean as a scalar times a sum, so the same properties are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24443884729220428\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(Y: np.array):\n",
    "    res = np.zeros((Y.size, Y.max() + 1))\n",
    "    res[np.arange(Y.size), Y] = 1\n",
    "    return res.T\n",
    "\n",
    "\n",
    "def cross_entropy_loss(predicted_probs, one_hot_labels):\n",
    "    Y_hat, Y = predicted_probs, one_hot_labels\n",
    "    avoid_divide_by_zero_term = 1e-8\n",
    "    return -np.mean(np.log(Y_hat + avoid_divide_by_zero_term) * Y)\n",
    "\n",
    "Y_truth_one_hot = to_one_hot(Y_train)\n",
    "print(cross_entropy_loss(Y_hat, Y_truth_one_hot))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Back-propagation\n",
    "\n",
    "Backpropagation's goal is to **determine how to change the weights and biases in order to minimize the cost function output.** \n",
    "In mathematical terms, we are calulating the gradient of C with respect to W2, b2, W1, and b1.\n",
    "\n",
    "Required math pre-reqs:\n",
    "\n",
    "- single-variable calculus: understand that the derivative is the rate that y changes with respect to x (how y changes when x changes)\n",
    "- multi-variable calculus: chain rule\n",
    "- multi-variable calculus: partial derivites and gradient vector (for every variable, assume all other variables are constant, store the per-variable results in a vector)\n",
    "\n",
    "First, let's calculate how the loss changes with respect to the weights and biases. Translating aforementioned sentence to math, we arrive at the below formulas:\n",
    "\n",
    "$ C = cost function(W^{[2]}, B^{[2]}, W^{[1]}, B^{[1]}) $\n",
    "\n",
    "$ dW^{[2]} = \\frac{\\partial C}{\\partial W^{[2]}} = \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} * \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} $\n",
    "\n",
    "$ dB^{[2]} = \\frac{\\partial C}{\\partial B^{[2]}} = \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} * \\frac{\\partial Z^{[2]}}{\\partial B^{[2]}} $\n",
    "\n",
    "$ dW^{[1]} = \\frac{\\partial C}{\\partial W^{[1]}} = \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} * \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} * \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} * \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}$\n",
    "\n",
    "$ dB^{[1]} = \\frac{\\partial C}{\\partial B^{[1]}} = \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} * \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} * \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} * \\frac{\\partial Z^{[1]}}{\\partial B^{[1]}}$\n",
    "\n",
    "Next, let's calculate the value of each of the individual terms:\n",
    "\n",
    "---\n",
    "**dW2, and db2**\n",
    "\n",
    "$ dW^{[2]} = \\frac{\\partial C}{\\partial W^{[2]}} = \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} * \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} $\n",
    "\n",
    "We can compute dW2 by first evaluting each partial derivate in above formula, and multiplying them together.\n",
    "\n",
    "$ \\frac{\\partial C}{\\partial Y_{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} $ simplifies super nicely into $ Y_{hat} - Y_{truth} $, the derivation is detailed in this [video](https://www.youtube.com/watch?v=5-rVLSc2XdE). Here are some good written resources: [link](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/), [link](https://deepnotes.io/softmax-crossentropy#cross-entropy-loss), [link](https://themaverickmeerkat.com/2019-10-23-Softmax/).\n",
    "\n",
    "$ \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}} $ is the partial derivative of $ W2 * A1 + B1 wrt W2 $, which evaluates to $ A1 $\n",
    "\n",
    "Hence, dW2 = (Y_hat - Y_truth) * A1\n",
    "\n",
    "Similarly, dB2 = (Y_hat - Y_truth) * partial_deriv(Z2 wrt B) = (Y_hat - Y_truth) * 1\n",
    "\n",
    "In our implementation, we need to take the sum of dB2 in order to match the dimensions with B2.\n",
    "\n",
    "---\n",
    "\n",
    "**dW1, and db1**\n",
    "\n",
    "We can reuse the $ \\frac{\\partial C}{\\partial Y*{hat}} * \\frac{\\partial Y_{hat}}{\\partial Z^{[2]}} $ term calculated above. The partial derivate of Z1 wrt to W1, wrt B1, and wrt A1 are very similar to dW2 and dB2 above. The new term is $ \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} $, which is the partial derivative of the ReLU activation function (good resource [here](https://kawahara.ca/what-is-the-derivative-of-relu/)).\n",
    "\n",
    "---\n",
    "\n",
    "**An Extra 1/m term**\n",
    "\n",
    "We elected to modify the cross entropy loss function by replacing the sum operator with the mean operator. As a result, the resulting derivatives will have a constant of 1/m. dW2, db2, dW1, and db1 should all be multiplied by 1/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10) (10, 1) (10, 784) (10, 1)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid_derivative(input):\n",
    "    s = 1 / (1 + np.exp(input))\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "num_train_examples = num_examples - validation_data_size\n",
    "def backward(Y_truth_one_hot, Y_hat, Z2, A1, Z1, W2, W1, X):\n",
    "    dZ2 = Y_hat - Y_truth_one_hot\n",
    "    dW2 = 1 / num_train_examples * dZ2.dot(A1.T)\n",
    "    db2 = 1 / num_train_examples * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = W2.T.dot(dZ2) * sigmoid_derivative(Z1)\n",
    "    dW1 = 1 / num_train_examples * dZ1.dot(X.T)\n",
    "    db1 = 1 / num_train_examples * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return dW2, db2, dW1, db1\n",
    "\n",
    "\n",
    "dW2, db2, dW1, db1 = backward(to_one_hot(Y_train), Y_hat, Z2, A1, Z1, W2, W1, X_train)\n",
    "print(dW2.shape, db2.shape, dW1.shape, db1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Update Params (take a small step in optimal direction)\n",
    "\n",
    "The gradient (all the above partial derivaties) informs us on the best direction to move in, but we don't know how far. Thus, in gradient descent, we take frequent steps, re-evaluate, and repeat. The step size is called the learning rate. Small steps can lead to longer times to converge, and overly large steps may oscillate around the optimal point (too far left in one iteration, overcorrects by too much and is too far right in the next, and then too far left again).\n",
    "\n",
    "The learning rate is an instance of a **hyper-parameter**, which is a paramater the model cannot learn. Practitioners set this value using existing heuristics.\n",
    "\n",
    "Here is an example of how we would update the weights:\n",
    "\n",
    "```\n",
    "W2 = W2 - learning_rate * dW2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n",
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "def update_params(W2, b2, W1, b1, dW2, db2, dW1, db1, learning_rate):\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    return W2, b2, W1, b1\n",
    "\n",
    "\n",
    "W2_new, b2_new, W1_new, b1_new = update_params(W2, b2, W1, b1, dW2, db2, dW1, db1, 0.10)\n",
    "print(np.unique(W1_new == W1))\n",
    "print(dW1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Repeate until convergence\n",
    "\n",
    "In this implementation, we will run a set number of iterations.\n",
    "\n",
    "# Let's start model training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "accuracy:  0.10559375\n",
      "Iteration:  100\n",
      "accuracy:  0.37753125\n",
      "Iteration:  200\n",
      "accuracy:  0.53871875\n",
      "Iteration:  300\n",
      "accuracy:  0.61465625\n",
      "Iteration:  400\n",
      "accuracy:  0.67065625\n",
      "Iteration:  500\n",
      "accuracy:  0.71353125\n",
      "Iteration:  600\n",
      "accuracy:  0.7429375\n",
      "Iteration:  700\n",
      "accuracy:  0.76665625\n",
      "Iteration:  800\n",
      "accuracy:  0.78621875\n",
      "Iteration:  900\n",
      "accuracy:  0.80228125\n"
     ]
    }
   ],
   "source": [
    "def grad_descent(X, Y, learning_rate, num_iters):\n",
    "    Y_truth_one_hot = to_one_hot(Y)\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(num_iters):\n",
    "        Z1, A1, Z2, Y_hat = forward(X, W1, b1, W2, b2)\n",
    "        dW2, db2, dW1, db1 = backward(Y_truth_one_hot, Y_hat, Z2, A1, Z1, W2, W1, X)\n",
    "        W2, b2, W1, b1 = update_params(\n",
    "            W2, b2, W1, b1, dW2, db2, dW1, db1, learning_rate\n",
    "        )\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            Y_hat_hum = get_human_readable_prediction(Y_hat)\n",
    "            Y_truth_hum = get_human_readable_prediction(Y_truth_one_hot)\n",
    "            print(\"Iteration: \", i)\n",
    "            print(\"accuracy: \", get_accuracy(Y_hat_hum, Y_truth_hum))\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "W1, b1, W2, b2 = grad_descent(X_train, Y_train, 0.10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.81540625\n"
     ]
    }
   ],
   "source": [
    "Z1, A1, Z2, Y_hat = forward(X_train, W1, b1, W2, b2)\n",
    "print(\"Train Accuracy: \", get_accuracy(get_human_readable_prediction(Y_hat), Y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  0.8112\n"
     ]
    }
   ],
   "source": [
    "Z1, A1, Z2, Y_hat = forward(X_val, W1, b1, W2, b2)\n",
    "print(\"Validation Accuracy: \", get_accuracy(get_human_readable_prediction(Y_hat), Y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We did it!\n",
    "\n",
    "We have implemente a MLP from scratch and trained a performant digit recognition model! How to increase accuracy even further?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Resources\n",
    "\n",
    "1. [3blue1brown neural networks playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "2. [Free Online Text Book on Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "3. [Understanding Backprop in Neural Networks with Basic Calculs](https://www.youtube.com/watch?v=wqPt3qjB6uA)\n",
    "4. [Gradient Descent | Neural Networks](https://www.youtube.com/watch?v=YS_EztqZCD8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89027c873307ecb527d3244bb63921688d806540739c227c0ed363c32ef772ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
